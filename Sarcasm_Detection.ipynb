{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"NLP_Project_Sarcasm_Detection_Questions.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"pp68FAQf9aMN"},"source":["# Sarcasm Detection\n"," **Acknowledgement**\n","\n","Misra, Rishabh, and Prahal Arora. \"Sarcasm Detection using Hybrid Neural Network.\" arXiv preprint arXiv:1908.07414 (2019).\n","\n","**Required Files given in below link.**\n","\n","https://drive.google.com/drive/folders/1xUnF35naPGU63xwRDVGc-DkZ3M8V5mMk"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"S3Wj_mIZ8S3K"},"source":["## Install `Tensorflow2.0` "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"jW2Uk8otQvi8","colab":{}},"source":["!!pip uninstall tensorflow\n","!pip install tensorflow==2.0.0"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"v9kv9tyJ77eF"},"source":["## Get Required Files from Drive"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"D0O_n6OIEVyL","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"0mgRpOvFMjKR","colab":{}},"source":["#Set your project path \n","project_path =  '/content/drive/My Drive/ML_Project/NLP/NLP-2 Sarcasm Detection/'"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"WXYwajPeQbRq"},"source":["#**## Reading and Exploring Data**"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"vAk6BRUh8CqL"},"source":["## Read Data \"Sarcasm_Headlines_Dataset.json\". Explore the data and get  some insights about the data. ( 4 marks)\n","Hint - As its in json format you need to use pandas.read_json function. Give paraemeter lines = True."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"StSLB-T8PuGr","colab":{}},"source":["import pandas as pd\n","import os\n","\n","data = pd.read_json(os.path.join(project_path,'Sarcasm_Headlines_Dataset.json'),lines=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"stjT3uHwGHPV","colab_type":"code","outputId":"979dd2e1-8789-4752-8577-b9a9e4981dde","executionInfo":{"status":"ok","timestamp":1585462762025,"user_tz":-330,"elapsed":5881,"user":{"displayName":"Kowshik Preetham","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgeFl4MG61h8mxXKWa_qiH_U2qr4iiQL06D62sr=s64","userId":"00492992892680713631"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["for i in range(0,11):\n","  print(data['headline'][i])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["former versace store clerk sues over secret 'black code' for minority shoppers\n","the 'roseanne' revival catches up to our thorny political mood, for better and worse\n","mom starting to fear son's web series closest thing she will have to grandchild\n","boehner just wants wife to listen, not come up with alternative debt-reduction ideas\n","j.k. rowling wishes snape happy birthday in the most magical way\n","advancing the world's women\n","the fascinating case for eating lab-grown meat\n","this ceo will send your kids to school, if you work for his company\n","top snake handler leaves sinking huckabee campaign\n","friday's morning email: inside trump's presser for the ages\n","airline passengers tackle man who rushes cockpit in bomb threat\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vSfqa4wkGVJ4","colab_type":"text"},"source":["**OBSERVATION** :\n","As seen in the above console, the text in 'headline' needs to be formatted.\n","Special characters and numbers needs to be removed from the text"]},{"cell_type":"code","metadata":{"id":"AF-o6DM0MQgo","colab_type":"code","outputId":"bfa53a9d-e93f-4760-bfb8-586f111cd638","executionInfo":{"status":"ok","timestamp":1585462841311,"user_tz":-330,"elapsed":1646,"user":{"displayName":"Kowshik Preetham","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgeFl4MG61h8mxXKWa_qiH_U2qr4iiQL06D62sr=s64","userId":"00492992892680713631"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["import re\n","from nltk.corpus import stopwords\n","import nltk\n","import string\n","nltk.download('stopwords')\n","stopwords = set(stopwords.words('english'))\n","def cleanData(text):\n","  text = re.sub(r'\\d+', '', text)\n","  text = \"\".join([char for char in text if char not in string.punctuation])\n","  #text = \" \".join([word for word in text.split(' ') if word not in stopwords]) \n","  #--> Model performs better without removing stop words, \n","  #    since words like 'not' will get removed which are essential for classification\n","  return text"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"onM71U1xPw-g","colab_type":"code","colab":{}},"source":["data['headline']=data['headline'].apply(cleanData)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"z6pXf7A78E2H"},"source":["## Drop `article_link` from dataset. ( 2 marks)\n","As we only need headline text data and is_sarcastic column for this project. We can drop artical link column here."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"VLSVsvrlP9qD","colab":{}},"source":["data.drop('article_link',inplace=True,axis=1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"D0h6IOxU8OdH"},"source":["## Get the Length of each line and find the maximum length. ( 4 marks)\n","As different lines are of different length. We need to pad the our sequences using the max length."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"BRAsChZAQmr3","colab":{}},"source":["maxlen = max([len(text) for text in data['headline']])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"VPPd0YuPXi2M"},"source":["#**## Modelling**"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"35abKfRx8as3"},"source":["## Import required modules required for modelling."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"DVel73hYEV4r","colab":{}},"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Flatten, Bidirectional, GlobalMaxPool1D\n","from tensorflow.keras.models import Model, Sequential"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"9ziybaD1RdD9"},"source":["# Set Different Parameters for the model. ( 2 marks)"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"jPw9gAN_EV6m","colab":{}},"source":["max_features = 10000\n","embedding_size = 200"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"9abSe-bM8fn9"},"source":["## Apply Keras Tokenizer of headline column of your data.  ( 4 marks)\n","Hint - First create a tokenizer instance using Tokenizer(num_words=max_features) \n","And then fit this tokenizer instance on your data column df['headline'] using .fit_on_texts()"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"T9Ad26HfTFMS","colab":{}},"source":["tokenizer = Tokenizer(num_words=max_features,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',lower=True,split=' ', char_level=False)\n","\n","tokenizer.fit_on_texts(data['headline'])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"0Ffi63KsST3P"},"source":["# Define X and y for your model."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"wnjxBdqmSS4s","colab":{}},"source":["X = tokenizer.texts_to_sequences(data['headline'])\n","X = pad_sequences(X, maxlen = maxlen)\n","y = np.asarray(data['is_sarcastic'])\n","\n","print(\"Number of Samples:\", len(X))\n","print(X[0])\n","print(\"Number of Labels: \", len(y))\n","print(y[0])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"WJLyKg-98rH_"},"source":["## Get the Vocabulary size ( 2 marks)\n","Hint : You can use tokenizer.word_index."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"q-2w0gHEUUIo","colab":{}},"source":["num_words=len(tokenizer.word_index)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"5hjeMi40XcB1"},"source":["#**## Word Embedding**"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"bUF1TuQa8ux0"},"source":["## Get Glove Word Embeddings"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"vq5AIfRtMeZh","colab":{}},"source":["glove_file = \"/content/drive/My Drive/ML_Project/NLP/NLP-1 Sentiment Classification/glove.6B.zip\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"DJLX_n2WMecA","colab":{}},"source":["#Extract Glove embedding zip file\n","from zipfile import ZipFile\n","with ZipFile(glove_file, 'r') as z:\n","  z.extractall()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"9IuXlu8-U3HG"},"source":["# Get the Word Embeddings using Embedding file as given below."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"elZ-T5aFGZmZ","colab":{}},"source":["EMBEDDING_FILE = './glove.6B.200d.txt'\n","\n","embeddings = {}\n","for o in open(EMBEDDING_FILE):\n","    word = o.split(\" \")[0]\n","    # print(word)\n","    embd = o.split(\" \")[1:]\n","    embd = np.asarray(embd, dtype='float32')\n","    # print(embd)\n","    embeddings[word] = embd\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"bTPxveDmVCrA"},"source":["# Create a weight matrix for words in training docs"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"xQgOhiywU9nU","outputId":"1ea2433d-0a09-4b2c-9487-b53ea10b0707","executionInfo":{"status":"ok","timestamp":1585463019042,"user_tz":-330,"elapsed":2495,"user":{"displayName":"Kowshik Preetham","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgeFl4MG61h8mxXKWa_qiH_U2qr4iiQL06D62sr=s64","userId":"00492992892680713631"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["embedding_matrix = np.zeros((num_words+1, 200))\n","\n","for word, i in tokenizer.word_index.items():\n","    embedding_vector = embeddings.get(word)\n","    if embedding_vector is not None:\n","        embedding_matrix[i] = embedding_vector\n","\n","len(embeddings.values())"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["400000"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"u7IbWuEX82Ra"},"source":["## Create and Compile your Model  ( 7 marks)\n","Hint - Use Sequential model instance and then add Embedding layer, Bidirectional(LSTM) layer, then dense and dropout layers as required. \n","In the end add a final dense layer with sigmoid activation for binary classification.\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"d7jhsSgYXG4l","colab":{}},"source":["### Embedding layer for hint \n","## model.add(Embedding(num_words, embedding_size, weights = [embedding_matrix]))\n","### Bidirectional LSTM layer for hint \n","## model.add(Bidirectional(LSTM(128, return_sequences = True)))\n","\n","input_layer = Input(shape=(maxlen,),dtype=tf.int64)\n","embed = Embedding(embedding_matrix.shape[0],output_dim=200,weights=[embedding_matrix],input_length=maxlen, trainable=True)(input_layer)\n","lstm=Bidirectional(LSTM(128))(embed)\n","drop=Dropout(0.3)(lstm)\n","dense =Dense(100,activation='relu')(drop)\n","out=Dense(2,activation='softmax')(dense)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"IJFMxZwMWoTw"},"source":["# Fit your model with a batch size of 100 and validation_split = 0.2. and state the validation accuracy ( 5 marks)\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ZpVkajCcWnRK","outputId":"1f2e7ab9-83bf-4ede-f4dc-de2efd5d5644","executionInfo":{"status":"ok","timestamp":1585463039277,"user_tz":-330,"elapsed":900,"user":{"displayName":"Kowshik Preetham","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgeFl4MG61h8mxXKWa_qiH_U2qr4iiQL06D62sr=s64","userId":"00492992892680713631"}},"colab":{"base_uri":"https://localhost:8080/","height":357}},"source":["batch_size = 100\n","epochs = 5\n","\n","model = Model(input_layer,out)\n","\n","model.compile(loss='sparse_categorical_crossentropy',optimizer=\"adam\",metrics=['accuracy'])\n","model.summary()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Model: \"model\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_1 (InputLayer)         [(None, 240)]             0         \n","_________________________________________________________________\n","embedding (Embedding)        (None, 240, 200)          5533600   \n","_________________________________________________________________\n","bidirectional (Bidirectional (None, 256)               336896    \n","_________________________________________________________________\n","dropout (Dropout)            (None, 256)               0         \n","_________________________________________________________________\n","dense (Dense)                (None, 100)               25700     \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 2)                 202       \n","=================================================================\n","Total params: 5,896,398\n","Trainable params: 5,896,398\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_jPFXJ4PdJ_Z","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ffOhcqDKdx3V","colab_type":"code","outputId":"bdf50028-124d-4321-e827-11f9cd7c0e72","executionInfo":{"status":"ok","timestamp":1585463156612,"user_tz":-330,"elapsed":103524,"user":{"displayName":"Kowshik Preetham","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgeFl4MG61h8mxXKWa_qiH_U2qr4iiQL06D62sr=s64","userId":"00492992892680713631"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["model.fit(X_train,y_train,batch_size=batch_size, epochs=epochs, verbose=1)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch 1/5\n","214/214 [==============================] - 19s 88ms/step - loss: 0.4498 - accuracy: 0.7803\n","Epoch 2/5\n","214/214 [==============================] - 19s 87ms/step - loss: 0.2659 - accuracy: 0.8921\n","Epoch 3/5\n","214/214 [==============================] - 19s 86ms/step - loss: 0.1745 - accuracy: 0.9331\n","Epoch 4/5\n","214/214 [==============================] - 18s 86ms/step - loss: 0.1134 - accuracy: 0.9583\n","Epoch 5/5\n","214/214 [==============================] - 18s 86ms/step - loss: 0.0796 - accuracy: 0.9693\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f4510233390>"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"code","metadata":{"id":"urwcKMwixj0C","colab_type":"code","outputId":"ee04e534-917a-4a45-bfd8-5b4515039ee9","executionInfo":{"status":"ok","timestamp":1585463165128,"user_tz":-330,"elapsed":3376,"user":{"displayName":"Kowshik Preetham","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgeFl4MG61h8mxXKWa_qiH_U2qr4iiQL06D62sr=s64","userId":"00492992892680713631"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["test_pred = model.predict(np.array(X_test), verbose=1)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["167/167 [==============================] - 2s 11ms/step\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fRPUAsM8yDKv","colab_type":"code","colab":{}},"source":["test_pred = [1 if j>i else 0 for i,j in test_pred]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9PgiB25B-CG8","colab_type":"code","outputId":"4fa930cb-8f82-452e-d499-db0984247c56","executionInfo":{"status":"ok","timestamp":1585463177607,"user_tz":-330,"elapsed":1769,"user":{"displayName":"Kowshik Preetham","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgeFl4MG61h8mxXKWa_qiH_U2qr4iiQL06D62sr=s64","userId":"00492992892680713631"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["from sklearn.metrics import confusion_matrix\n","confusion_matrix(y_test, test_pred)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[2671,  360],\n","       [ 372, 1939]])"]},"metadata":{"tags":[]},"execution_count":36}]},{"cell_type":"code","metadata":{"id":"YZTGY32B--jZ","colab_type":"code","outputId":"da7528ce-b823-4902-c44f-39f0f6ecaeb5","executionInfo":{"status":"ok","timestamp":1585463445530,"user_tz":-330,"elapsed":1552,"user":{"displayName":"Kowshik Preetham","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgeFl4MG61h8mxXKWa_qiH_U2qr4iiQL06D62sr=s64","userId":"00492992892680713631"}},"colab":{"base_uri":"https://localhost:8080/","height":170}},"source":["from sklearn.metrics import classification_report\n","\n","print(classification_report(y_test, test_pred))\n","# Accuracy can still be increased by increasing the number of epochs"],"execution_count":0,"outputs":[{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.88      0.88      0.88      3031\n","           1       0.84      0.84      0.84      2311\n","\n","    accuracy                           0.86      5342\n","   macro avg       0.86      0.86      0.86      5342\n","weighted avg       0.86      0.86      0.86      5342\n","\n"],"name":"stdout"}]}]}